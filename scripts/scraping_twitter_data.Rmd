---
title: Scraping Twitter Data
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
 knitr::opts_chunk$set(eval = FALSE)
```

In the article below, I walk through our process for collecting data using Twitter's API. 

>NOTE: I collected data prior to Twitter changing API access. As such, much (if not all) of this information may not be useful to you if you wish to use Twitter data in your own research. At the time of writing this, Twitter has proposed that access to the API may cost $42,000 per month - way too much for a measly graduate student like me.'

I secured API access by applying for a researcher account. This entailed writing a short blurb about this project and agreeing to terms/conditions for data privacy (as well as promising I'm not a corporation). The researcher account then gave me a *bearer token* - a unique code that acted as our key to get into the API from R. 
In R, I scraped Tweets using the [academictwitteR](https://github.com/cjbarrie/academictwitteR) package. 
I first added our bearer token to the R environment using the function

```{r}
set_bearer()
```

I then wrote a wrapper function to integrate several functions from the academictwitteR package. 

```{r}
get_me_some_tweets <- function(QUERY_TEXT, DATA_LOCATION, get_new = TRUE, resume = FALSE, update = FALSE, end_time = Sys.time()){
  
  # Set Variables
  
  DATE <- end_time

  require(academictwitteR)
  
  if(get_new == TRUE){
    
    # Set Query
    
    QUERY <- build_query(
      query = QUERY_TEXT,
      is_retweet = FALSE,
      lang = "en"
    )
  
    # Collect Tweets
    
    OUTPUT <-
      get_all_tweets(
        query = QUERY,
        start_tweets = "2007-01-01T00:00:00Z", # This is the beginning of Twitter
        end_tweets = DATE,
        data_path = DATA_LOCATION,
        bind_tweets = FALSE,
        n = Inf,
        export_query = TRUE
      )
  }else if(resume == TRUE){
    
    # Resume Collection
    
    resume_collection(data_path = DATA_LOCATION,
                      n = Inf,
                      bind_tweets = FALSE)
  }else if(update == TRUE){
    
    # Update Collection
    update_collection(data_path = DATA_LOCATION, 
                      end_tweets = DATE)
  }
}
```

Below is a brief walk through of the arguments for this function in an example query pulling Tweets cancelling Kanye (Ye) West. The search terms are saved as QUERY_TEXT and a folder location for the .json files is saved as DATA_LOCATION.

```{r}
QUERY_TEXT = c("#FuckYe", "#kanyewestcancelled", "#cancelkanyewest",
            "kanyeisoverparty", "cancelkanye", "kanyecancelled")

DATA_LOCATION = here::here("kanye/")

get_me_some_tweets(QUERY_TEXT, DATA_LOCATION)
```

There are then three additional arguments that you can specify:

```{r}
get_new = TRUE
resume = FALSE 
update = FALSE
```

get_new starts a new search and pulls Tweets from the beginning of Twitter to the current day. Setting resume = TRUE and get_new = FALSE allows you to resume a prior search from the most recent Tweet. A query can take between 45 minutes and 16 hours to complete. If your connection is interrupted, you can then resume from where you left off. Finally, update = TRUE acts similar to resume = TRUE but instead starts scraping Tweets from the most recent in the current folder. This allows me to go back and get more recent Tweets after the first scrape.

After scraping Tweets you'll be left with a folder of loose .json files. We then bind these tweets into a data frame using the bind_tweets function

```{r}
kanye <- bind_tweets(data_path = "kanye/", output_format = "tidy")
```

For this app, I used the following packages:

```{r}
# For loading and combining multiple files
library(vroom)
files <- paste0("static/data/LIWC_cc", 1:3, ".csv")
data <- vroom(files)
```

In this post, I'll walk us through our process for cleaning these tweets. The data we're using here are every mention of the words *cancel culture* in a tweet from the beginning of Twitter (2007). When preparing the API call, we removed all retweets and searched for only posts in English. This has given us `r length(data)` posts.

We initially ran our data through the LIWC-22 software. You can read more about LIWC [here](https://www.liwc.app/).

This gave us numeric scores for over 100 different linguistic components ranging from analytic thinking to the rate of I/Me pronouns. From these, we've narrowed down to a smaller list of 32 theoretically-related aspects.

```{r}
data <- janitor::clean_names(data) %>% 
  select(
    tweet_id, text, user_verified, retweet_count, like_count, quote_count, user_followers_count, user_following_count, 

wc, analytic, clout, authentic,  big_words,
they, ipron, number, affiliation, achieve, power, cognition, allnone, cogproc, insight, 
cause, discrep, tentat, certitude, tone_pos,
tone_neg, emo_anx, emo_anger, emo_sad,
swear, socbehav, prosocial, polite, conflict,
moral, politic, curiosity)
names(data) = useful_names[1:40]
```

We next ensure that our data are actually in English. The Twitter API's language filter options are very imperfect.

```{r}
data <- data %>% 
  filter(cld3::detect_language(text) == "en")
```

Finally, we save our data in a .RDS file to ease in manipulating it over the next several apps.

```{r}
saveRDS(data, here::here("static/data/cc.RDS"))
```


